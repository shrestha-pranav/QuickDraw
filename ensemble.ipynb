{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ensemble",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "ce6d2aa7de1fa341144def7d3a5b1ffdea26bc91",
        "id": "Yu5oibox58pv",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Dependencies\n",
        "%matplotlib inline\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import ast\n",
        "import os\n",
        "import json\n",
        "import datetime as dt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import cv2\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [16, 10]\n",
        "plt.rcParams['font.size'] = 14\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import concatenate, Input, Conv2D, MaxPooling2D\n",
        "from keras.layers import GlobalAveragePooling2D, LSTM, Bidirectional, Conv1D, BatchNormalization, Dense, Dropout, Flatten, Activation\n",
        "\n",
        "from keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.applications import MobileNet\n",
        "from keras.applications.mobilenet import preprocess_input\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "start = dt.datetime.now()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9DNJJpXV-fq5",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Auth for GDrive\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mDe3Khqi9k0a",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Download npy files from GDrive\n",
        "\n",
        "file_ids = {\n",
        "}\n",
        "\n",
        "import numpy as np\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "data = {}\n",
        "\n",
        "for file_name, file_id in file_ids.items():\n",
        "    request = drive_service.files().get_media(fileId=file_id) \n",
        "    downloaded = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(downloaded, request)\n",
        "    done = False\n",
        "\n",
        "    while done is False:\n",
        "        status, done = downloader.next_chunk()\n",
        "        print('Download %d%%.' % int(status.progress() * 100))\n",
        "        \n",
        "    downloaded.seek(0)\n",
        "    data[file_name] = np.load(downloaded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BtwMKNJ-Axc2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Code: Directory Downloader {display-mode: \"form\"}\n",
        "\n",
        "# This code will be hidden when the notebook is loaded.\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "def download_drive_dir(local_dir, folder_id, whitelist=[]):\n",
        "    \"\"\"\n",
        "    params:\n",
        "        local_dir: Colaboratory directory\n",
        "        folder_id: Google Drive folder ID\n",
        "    \"\"\"\n",
        "    local_download_path = os.path.expanduser(local_dir) \n",
        "    try:\n",
        "        os.makedirs(local_download_path)\n",
        "    except Exception as e:\n",
        "        print('Error creating path:', e)\n",
        "\n",
        "    file_list = drive.ListFile(\n",
        "        {'q': \"'{}' in parents\".format(folder_id)}).GetList()\n",
        "\n",
        "    for f in tqdm(file_list):\n",
        "        if not len(whitelist) or f['title'] in whitelist:\n",
        "            # print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "            fname = os.path.join(local_download_path, f['title'])\n",
        "            # print('downloading to {}'.format(fname))\n",
        "            f_ = drive.CreateFile({'id': f['id']})\n",
        "            f_.GetContentFile(fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pvw8H8EHA3T5",
        "colab_type": "code",
        "outputId": "d147ffd7-1563-4580-e358-92c9e29d6948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "download_drive_dir('./models', '1Fjv7uXEc92yrbg7drIryB4ZdtOp-aLgQ')\n",
        "download_drive_dir('./pretrained_models', '1swZb9ootRNXeSXR7iaTmVZsccsRK-UW9')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error creating path: [Errno 17] File exists: './models'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 23/23 [00:19<00:00,  1.11it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Error creating path: [Errno 17] File exists: './pretrained_models'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:15<00:00,  1.17it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "H2Li6sOMeZWF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "download_drive_dir('./input/shuffle-csvs', '1H8ogDcbBGsgAJkxaOxXd-XR3ZH14su3i')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "matyT9KoDQWj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NCATS = 340\n",
        "image_size = 128\n",
        "size = 128\n",
        "time_color = True\n",
        "stroke_size = 3\n",
        "DP_DIR = './input/shuffle_csvs/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G7Wy-mrAC1Xm",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Utils\n",
        "sampler = {image_size:[round(i*(image_size-10)/256)+5 for i in range(512)] for image_size in [128, 224, 299, 331]}\n",
        "stroke_color = defaultdict(lambda: 125 if time_color else 255,\n",
        "        {t:255-min(t,10)*13 for t in range(10)} if time_color else {})\n",
        "def f2cat(filename): return filename.split('.')[0]\n",
        "def list_all_categories(): return sorted([f2cat(f) for f in os.listdir(os.path.join(INPUT_DIR, 'train_simplified'))], key=str.lower)\n",
        "def apk(actual, predicted, k=3):\n",
        "    if len(predicted) > k: predicted = predicted[:k]\n",
        "    score, num_hits = 0.0, 0.0\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "    if len(actual)==0: return 0.0\n",
        "    return score / min(len(actual), k)\n",
        "def mapk(actual, predicted, k=3): return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
        "def preds2catids(predictions): return pd.DataFrame(np.argsort(-predictions, axis=1)[:, :3], columns=['a', 'b', 'c'])\n",
        "def top_3_accuracy(y_true, y_pred): return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
        "log_keys = ('loss', 'val_loss', 'categorical_accuracy', 'top_3_accuracy', 'val_categorical_accuracy', 'val_top_3_accuracy')\n",
        "def print_history(hists): print(\"\\n\".join([\"\\t\".join([\"%.3f\"%hist.history[key][-1] for key in log_keys]) for hist in hists]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s_95OX6xC_iO",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Image processing utils\n",
        "def _stack_it(stroke_vec):\n",
        "    \"\"\"preprocess the string and make a standard Nx3 stroke vector\"\"\"\n",
        "    # unwrap the list\n",
        "    in_strokes = [(xi,yi,i)\n",
        "          for i,(x,y) in enumerate(stroke_vec)\n",
        "          for xi,yi in zip(x,y)]\n",
        "    c_strokes = np.stack(in_strokes)\n",
        "\n",
        "    # replace stroke id with 1 for continue, 2 for new\n",
        "    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n",
        "    c_strokes[:,2] += 1 # since 0 is no stroke\n",
        "    \n",
        "    # pad the strokes with zeros\n",
        "    return pad_sequences(c_strokes.swapaxes(0, 1), \n",
        "                         maxlen=70, \n",
        "                         padding='post').swapaxes(0, 1)\n",
        "\n",
        "def draw_cv2(img, raw_strokes, image_size):\n",
        "    for t in range(len(raw_strokes)-1, -1, -1):\n",
        "        stroke = raw_strokes[t]\n",
        "        for i in range(len(stroke[0]) - 1):\n",
        "            cv2.line(img,\n",
        "                (int(sampler[image_size][stroke[0][i]]), int(sampler[image_size][stroke[1][i]])),\n",
        "                (int(sampler[image_size][stroke[0][i+1]]), int(sampler[image_size][stroke[1][i+1]])),\n",
        "                stroke_color[i], stroke_size)\n",
        "\n",
        "def sample(strokes, downsize_to=size-2):\n",
        "    division = 256 / downsize_to\n",
        "    for i in range(len(strokes)):\n",
        "        for j in range(len(strokes[i])):\n",
        "            for k in range(len(strokes[i][j])):\n",
        "                strokes[i][j][k] = round(strokes[i][j][k]/division)+1\n",
        "    return strokes\n",
        "\n",
        "def image_generator_xd(batchsize, ks, image_size):\n",
        "    while True:\n",
        "        for k in np.random.permutation(ks):\n",
        "            filename = os.path.join(DP_DIR, 'train_k{}.csv'.format(k))\n",
        "            for df in pd.read_csv(filename, chunksize=batchsize):\n",
        "                df.drawing = df.drawing.apply(json.loads)\n",
        "                \n",
        "                x = np.zeros((len(df), image_size, image_size, 1))\n",
        "                for i, raw_strokes in enumerate(df.drawing.values):\n",
        "                    draw_cv2(x[i], raw_strokes, image_size)\n",
        "                x = np.repeat(x, 3, axis=3)\n",
        "                x = preprocess_input(x).astype(np.float32)\n",
        "                \n",
        "                y = keras.utils.to_categorical(df.y, num_classes=NCATS)\n",
        "                yield x, y\n",
        "                \n",
        "def image_generator_test(filename, batchsize, image_size, repeat_vec=True):\n",
        "    for df in pd.read_csv(filename, chunksize=batchsize):\n",
        "        df.drawing = df.drawing.apply(json.loads)\n",
        "\n",
        "        x = np.zeros((len(df), image_size, image_size, 1))\n",
        "        for i, raw_strokes in enumerate(df.drawing.values):\n",
        "            draw_cv2(x[i], raw_strokes, image_size)\n",
        "        if repeat_vec:\n",
        "            x = np.repeat(x, 3, axis=3)\n",
        "        x = preprocess_input(x).astype(np.float32)\n",
        "\n",
        "        yield x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FgqRp2otB-4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('test_simplified.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iKYG3wtMTt_T",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title MobileNet RNN Model\n",
        "def mobilenet_rnn():\n",
        "    base_model = MobileNet(input_shape=(image_size, image_size, 1), alpha=1.,\n",
        "                      weights=None, dropout=0.2, classes=NCATS)\n",
        "#     base_model.load_weights(\"mobilenet.h5\")\n",
        "\n",
        "#     for layer in base_model.layers[:-12]:\n",
        "#         layer.trainable = False\n",
        "\n",
        "    base_model = Sequential(base_model.layers)\n",
        "\n",
        "    inp = Input(shape = (70,3))\n",
        "\n",
        "    x = BatchNormalization()(inp)\n",
        "\n",
        "    x = Conv1D(256, (5,), activation = \"relu\")(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Conv1D(256, (5,), activation = 'relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Conv1D(256, (3,), activation = 'relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Bidirectional(LSTM(128, return_sequences = True))(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Bidirectional(LSTM(128, return_sequences = False))(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Dense(512, activation = 'relu')(x)\n",
        "\n",
        "    stroke_read_model = Model(inp, x)\n",
        "    stroke_read_model = Sequential(stroke_read_model.layers)\n",
        "\n",
        "    inp1 = base_model.input\n",
        "    out1 = base_model.output\n",
        "\n",
        "    inp2 = Input(shape = (70, 3))\n",
        "    out2 = stroke_read_model(inp2)\n",
        "\n",
        "    x = concatenate([out1, out2])\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(NCATS, activation='softmax')(x)\n",
        "    model = Model([inp1, inp2], x)\n",
        "\n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9eJRrImZ58qS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_model(model_generator):\n",
        "    model = model_generator(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "    out = model.output\n",
        "    out = GlobalAveragePooling2D()(out)\n",
        "    out = Dense(1000, activation=\"relu\")(out)\n",
        "    out = Dense(NCATS, activation=\"softmax\")(out)\n",
        "\n",
        "    model = Model(model.input, out)\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy',\n",
        "                  metrics=[categorical_accuracy, top_3_accuracy])\n",
        "    return model\n",
        "\n",
        "from keras.applications import mobilenet, resnet50, xception, inception_v3\n",
        "\n",
        "_mobilenet_rnn = mobilenet_rnn()\n",
        "_mobilenet_rnn.load_weights(\"./models/MobilenetRNN.128.256.2000.02-022524.30-0.74.h5\")\n",
        "\n",
        "_mobilenet_2 = MobileNet(input_shape=(image_size, image_size, 1), alpha=1.,\n",
        "                  weights=None, dropout=0.2, classes=NCATS)\n",
        "_mobilenet_2.load_weights(\"./models/FastLoader.128.256.2000.10-0.75.h5\")\n",
        "\n",
        "_inception = generate_model(inception_v3.InceptionV3)\n",
        "_inception.load_weights(\"./pretrained_models/InceptionV3.02-071834-10-1.08.h5\")\n",
        "\n",
        "_mobilenet_1 = MobileNet(input_shape=(image_size, image_size, 1), alpha=1.,\n",
        "                  weights=None, dropout=0.2, classes=NCATS)\n",
        "_mobilenet_1.load_weights(\"./models/MobileNet.01-0.88.hdf5\")\n",
        "\n",
        "_mobilenet = generate_model(mobilenet.MobileNet)\n",
        "_mobilenet.load_weights(\"./pretrained_models/MobileNetv1.02-071834-10-1.01.h5\")\n",
        "\n",
        "_resnet50 = generate_model(resnet50.ResNet50)\n",
        "_resnet50.load_weights(\"./pretrained_models/ResNet50.02-071834-10-1.07.h5\")\n",
        "\n",
        "_xception = generate_model(xception.Xception)\n",
        "_xception.load_weights(\"./pretrained_models/Xception.02-071834-10-1.02.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bOLr3sSzbmUb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batchsize=1024\n",
        "        \n",
        "valid_datagen = image_generator_xd(batchsize, range(190, 200), 128)\n",
        "\n",
        "def predict(model, repeat_vec=True):\n",
        "    return model.predict_generator(\n",
        "        image_generator_test('./test_simplified.csv', batchsize, 128, repeat_vec=repeat_vec), \n",
        "        steps=int(len(test)/batchsize)+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CtoqtSuscpFk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "npy_mobilenet_rnn = predict(_mobilenet_rnn)\n",
        "np.save('mobilenet_rnn', npy_mobilenet_rnn); files.download('mobilenet_rnn.npy')\n",
        "npy_mobilenet_2 = predict(_mobilenet_2, repeat_vec=False)\n",
        "np.save('mobilenet_probas_2', npy_mobilenet); files.download('mobilenet_probas_2.npy')\n",
        "npy_inception = predict(_inception)\n",
        "np.save('inception_probas', npy_inception); files.download('inception_probas.npy')\n",
        "npy_mobilenet_1 = predict(_mobilenet_1, repeat_vec=False)\n",
        "np.save('mobilenet_probas_1', npy_mobilenet); files.download('mobilenet_probas_1.npy')\n",
        "npy_mobilenet = predict(_mobilenet)\n",
        "np.save('mobilenet_probas', npy_mobilenet); files.download('mobilenet_probas.npy')\n",
        "npy_xception = predict(_xception)\n",
        "np.save('xception_probas', npy_xception); files.download('xception_probas.npy')\n",
        "npy_resnet50 = predict(_resnet50)\n",
        "np.save('resnet_probas', npy_resnet50); files.download('resnet_probas.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SK0VPDfb_prv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create Submission\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "cats = ['airplane', 'alarm clock', 'ambulance', 'angel', 'animal migration', 'ant', 'anvil', 'apple', 'arm', 'asparagus', 'axe', 'backpack', 'banana', 'bandage', 'barn', 'baseball', 'baseball bat', 'basket', 'basketball', 'bat', 'bathtub', 'beach', 'bear', 'beard', 'bed', 'bee', 'belt', 'bench', 'bicycle', 'binoculars', 'bird', 'birthday cake', 'blackberry', 'blueberry', 'book', 'boomerang', 'bottlecap', 'bowtie', 'bracelet', 'brain', 'bread', 'bridge', 'broccoli', 'broom', 'bucket', 'bulldozer', 'bus', 'bush', 'butterfly', 'cactus', 'cake', 'calculator', 'calendar', 'camel', 'camera', 'camouflage', 'campfire', 'candle', 'cannon', 'canoe', 'car', 'carrot', 'castle', 'cat', 'ceiling fan', 'cell phone', 'cello', 'chair', 'chandelier', 'church', 'circle', 'clarinet', 'clock', 'cloud', 'coffee cup', 'compass', 'computer', 'cookie', 'cooler', 'couch', 'cow', 'crab', 'crayon', 'crocodile', 'crown', 'cruise ship', 'cup', 'diamond', 'dishwasher', 'diving board', 'dog', 'dolphin', 'donut', 'door', 'dragon', 'dresser', 'drill', 'drums', 'duck', 'dumbbell', 'ear', 'elbow', 'elephant', 'envelope', 'eraser', 'eye', 'eyeglasses', 'face', 'fan', 'feather', 'fence', 'finger', 'fire hydrant', 'fireplace', 'firetruck', 'fish', 'flamingo', 'flashlight', 'flip flops', 'floor lamp', 'flower', 'flying saucer', 'foot', 'fork', 'frog', 'frying pan', 'garden', 'garden hose', 'giraffe', 'goatee', 'golf club', 'grapes', 'grass', 'guitar', 'hamburger', 'hammer', 'hand', 'harp', 'hat', 'headphones', 'hedgehog', 'helicopter', 'helmet', 'hexagon', 'hockey puck', 'hockey stick', 'horse', 'hospital', 'hot air balloon', 'hot dog', 'hot tub', 'hourglass', 'house', 'house plant', 'hurricane', 'ice cream', 'jacket', 'jail', 'kangaroo', 'key', 'keyboard', 'knee', 'ladder', 'lantern', 'laptop', 'leaf', 'leg', 'light bulb', 'lighthouse', 'lightning', 'line', 'lion', 'lipstick', 'lobster', 'lollipop', 'mailbox', 'map', 'marker', 'matches', 'megaphone', 'mermaid', 'microphone', 'microwave', 'monkey', 'moon', 'mosquito', 'motorbike', 'mountain', 'mouse', 'moustache', 'mouth', 'mug', 'mushroom', 'nail', 'necklace', 'nose', 'ocean', 'octagon', 'octopus', 'onion', 'oven', 'owl', 'paint can', 'paintbrush', 'palm tree', 'panda', 'pants', 'paper clip', 'parachute', 'parrot', 'passport', 'peanut', 'pear', 'peas', 'pencil', 'penguin', 'piano', 'pickup truck', 'picture frame', 'pig', 'pillow', 'pineapple', 'pizza', 'pliers', 'police car', 'pond', 'pool', 'popsicle', 'postcard', 'potato', 'power outlet', 'purse', 'rabbit', 'raccoon', 'radio', 'rain', 'rainbow', 'rake', 'remote control', 'rhinoceros', 'river', 'roller coaster', 'rollerskates', 'sailboat', 'sandwich', 'saw', 'saxophone', 'school bus', 'scissors', 'scorpion', 'screwdriver', 'sea turtle', 'see saw', 'shark', 'sheep', 'shoe', 'shorts', 'shovel', 'sink', 'skateboard', 'skull', 'skyscraper', 'sleeping bag', 'smiley face', 'snail', 'snake', 'snorkel', 'snowflake', 'snowman', 'soccer ball', 'sock', 'speedboat', 'spider', 'spoon', 'spreadsheet', 'square', 'squiggle', 'squirrel', 'stairs', 'star', 'steak', 'stereo', 'stethoscope', 'stitches', 'stop sign', 'stove', 'strawberry', 'streetlight', 'string bean', 'submarine', 'suitcase', 'sun', 'swan', 'sweater', 'swing set', 'sword', 't-shirt', 'table', 'teapot', 'teddy-bear', 'telephone', 'television', 'tennis racquet', 'tent', 'The Eiffel Tower', 'The Great Wall of China', 'The Mona Lisa', 'tiger', 'toaster', 'toe', 'toilet', 'tooth', 'toothbrush', 'toothpaste', 'tornado', 'tractor', 'traffic light', 'train', 'tree', 'triangle', 'trombone', 'truck', 'trumpet', 'umbrella', 'underwear', 'van', 'vase', 'violin', 'washing machine', 'watermelon', 'waterslide', 'whale', 'wheel', 'windmill', 'wine bottle', 'wine glass', 'wristwatch', 'yoga', 'zebra', 'zigzag']\n",
        "\n",
        "id2cat = {k: cat.replace(' ', '_') for k, cat in enumerate(cats)}\n",
        "\n",
        "# test length x num categories\n",
        "m0 = np.load('mobilenet_probas_2.npy')\n",
        "m1 = np.load('mobilenet_probas_1.npy')\n",
        "m2 = np.load('xception_probas (1).npy')\n",
        "m3 = np.load('resnet_probas (1).npy')\n",
        "m4 = np.load('mobilenet_probas (1).npy')\n",
        "m5 = np.load('inception_probas.npy')\n",
        "\n",
        "# test_probas = [m0, m1, m2, m3, m4, m5]\n",
        "# weights = [92, 88, 74, 73, 74, 71]\n",
        "\n",
        "test_probas = [m0]\n",
        "weights = [92]\n",
        "\n",
        "test_probas = [a*b for a, b in zip(test_probas, weights)]\n",
        "M, N = test_probas[0].shape\n",
        "\n",
        "for arr in test_probas:\n",
        "    assert((M, N) == arr.shape)\n",
        "\n",
        "# Sum up the probabilities\n",
        "test_probas = np.array(test_probas)\n",
        "probas_sum = np.sum(test_probas, axis=0)\n",
        "\n",
        "predictions = []\n",
        "for i in range(M):\n",
        "    top_3 = np.argsort(probas_sum[i, :])[::-1][:3]# np.argpartition(probas_sum[i, :], -3)[:3]\n",
        "    top_3_cat = ' '.join(map(id2cat.get, top_3))\n",
        "    predictions.append(top_3_cat)\n",
        "\n",
        "test = pd.read_csv('./test_simplified.csv')\n",
        "test['word'] = pd.Series(predictions)\n",
        "submission = test[['key_id', 'word']]\n",
        "\n",
        "submission.head()\n",
        "submission.shape\n",
        "submission.to_csv('ensemble_submission.csv', index=False)\n",
        "\n",
        "files.download('ensemble_submission.csv')d\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}