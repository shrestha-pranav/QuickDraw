{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "522ce330-d4f9-4fbe-9684-4b65fd684cca",
    "_uuid": "174484daa5f084ce4970f5048d3e05d2c4429787"
   },
   "source": [
    "# Keras WaveNet Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "39077055d70da04344d314d12dba7f846a5e3721"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Conv1D, Input, Activation, AveragePooling1D, Add, Multiply, GlobalAveragePooling1D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DP_DIR = '../input/shuffle-csvs/'\n",
    "INPUT_DIR = '../input/quickdraw-doodle-recognition/'\n",
    "\n",
    "BASE_SIZE = 256\n",
    "NCSVS = 100\n",
    "NCATS = 340\n",
    "np.random.seed(seed=1987)\n",
    "tf.set_random_seed(seed=1987)\n",
    "def f2cat(filename: str) -> str:\n",
    "    return filename.split('.')[0]\n",
    "\n",
    "def list_all_categories():\n",
    "    files = os.listdir(os.path.join(INPUT_DIR, 'train_simplified'))\n",
    "    return sorted([f2cat(f) for f in files], key=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=3):\n",
    "    \"\"\" Taken from github/benhamner\"\"\"\n",
    "    if not actual: return 0.0\n",
    "    \n",
    "    score, num_hits = 0.0, 0.0\n",
    "    for i, p in enumerate(predicted[:k]):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=3):\n",
    "    \"\"\" Taken from github/benhamner \"\"\"\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "preds2catids   = lambda p: pd.DataFrame(np.argsort(-p, axis=1)[:,:3], columns=['a','b','c'])\n",
    "top_3_accuracy = lambda x,y: top_k_categorical_accuracy(x,y,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS  = 1000\n",
    "EPOCHS = 15\n",
    "size   = 80\n",
    "batchsize = 530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 128+64\n",
    "dilation_depth = 8\n",
    "activation = 'softmax'\n",
    "kernel_size = 2\n",
    "pool_size_1 = 4\n",
    "pool_size_2 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, i):\n",
    "    tanh_out = Conv1D(n_filters, \n",
    "                      kernel_size, \n",
    "                      dilation_rate = kernel_size**i, \n",
    "                      padding='causal', \n",
    "                      name='dilated_conv_%d_tanh' % (kernel_size ** i), \n",
    "                      activation='tanh'\n",
    "                      )(x)\n",
    "    sigm_out = Conv1D(n_filters, \n",
    "                      kernel_size, \n",
    "                      dilation_rate = kernel_size**i, \n",
    "                      padding='causal', \n",
    "                      name='dilated_conv_%d_sigm' % (kernel_size ** i), \n",
    "                      activation='sigmoid'\n",
    "                      )(x)\n",
    "    z = Multiply(name='gated_activation_%d' % (i))([tanh_out, sigm_out])\n",
    "    skip = Conv1D(n_filters, 1, name='skip_%d'%(i))(z)\n",
    "    res = Add(name='residual_block_%d' % (i))([skip, x])\n",
    "    return res, skip\n",
    "\n",
    "x = Input(shape=(size, size, 1), name='original_input')\n",
    "skip_connections = []\n",
    "out = Conv1D(n_filters, 2, dilation_rate=1, padding='causal', name='dilated_conv_1')(x)\n",
    "for i in range(1, dilation_depth + 1):\n",
    "    out, skip = residual_block(out,i)\n",
    "    skip_connections.append(skip)\n",
    "out = Add(name='skip_connections')(skip_connections)\n",
    "out = Activation('relu')(out)\n",
    "\n",
    "out = Conv1D(n_filters, pool_size_1, strides = 1, padding='same', name='conv_5ms', activation = 'relu')(out)\n",
    "out = AveragePooling1D(pool_size_1, padding='same', name='downsample_to_200Hz')(out)\n",
    "\n",
    "out = Conv1D(n_filters, pool_size_2, padding='same', activation='relu', name='conv_500ms')(out)\n",
    "out = Conv1D((340,), pool_size_2, padding='same', activation='relu', name='conv_500ms_target_shape')(out)\n",
    "out = AveragePooling1D(pool_size_2, padding='same',name = 'downsample_to_2Hz')(out)\n",
    "out = Conv1D((340,), (int) (input_shape[0] / (pool_size_1*pool_size_2)), padding='same', name='final_conv')(out)\n",
    "out = GlobalAveragePooling1D(name='final_pooling')(out)\n",
    "out = Activation(activation, name='final_activation')(out)\n",
    "\n",
    "model = Model(x, out)  \n",
    "model.compile(optimizer='adam',\n",
    "      loss='categorical_crossentropy', \n",
    "      metrics=['accuracy', top_3_accuracy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Image Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_cv2(raw_strokes, size=256, lw=6, time_color=True):\n",
    "    img = np.zeros((BASE_SIZE, BASE_SIZE), np.uint8)\n",
    "    for t, (p1, p2) in enumerate(raw_strokes):\n",
    "        for i in range(len(p1) - 1):\n",
    "            \n",
    "            # Temporal coloring scheme\n",
    "            color = 255 - min(t, 10) * 13 if time_color else 255\n",
    "            \n",
    "            # Add raw_stroke to image\n",
    "            _ = cv2.line(img, (p1[i], p2[i]), (p1[i+1], p2[i+1]), color, lw)\n",
    "\n",
    "    return img if size == BASE_SIZE else cv2.resize(img, (size, size))\n",
    "\n",
    "def ImageGenerator(size, batchsize, ks, lw=6, time_color=True):\n",
    "    while True:\n",
    "        for k in np.random.permutation(ks):\n",
    "            filename = os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(k))\n",
    "            for df in pd.read_csv(filename, chunksize=batchsize):\n",
    "                df['drawing'] = df['drawing'].apply(json.loads)\n",
    "                x = np.zeros((len(df), size, size, 1))\n",
    "                \n",
    "                for i, raw_strokes in enumerate(df.drawing.values):\n",
    "                    x[i,:,:,0] = draw_cv2(raw_strokes, size, lw, time_color)\n",
    "                x = preprocess_input(x).astype(np.float32)\n",
    "                y = keras.utils.to_categorical(df.y, num_classes=NCATS)\n",
    "                yield x, y\n",
    "\n",
    "def df_to_img(df, size, lw=6, time_color=True):\n",
    "    df['drawing'] = df['drawing'].apply(json.loads)\n",
    "    x = np.zeros((len(df), size, size, 1))\n",
    "    for i, raw_strokes in enumerate(df.drawing.values):\n",
    "        x[i,:,:,0] = draw_cv2(raw_strokes, size, lw, time_color)\n",
    "    x = preprocess_input(x).astype(np.float32)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv(os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(NCSVS - 1)), nrows=60000)\n",
    "x_valid  = df_to_img(valid_df, size)\n",
    "y_valid  = keras.utils.to_categorical(valid_df.y, num_classes=NCATS)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print('Validation array memory {:.2f} GB'.format(x_valid.nbytes / 1024.**3 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageGenerator(size=size, batchsize=batchsize, ks=range(NCSVS - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2a549512-a9d9-4afd-b748-3e1c3296e193",
    "_uuid": "5fda10b30c47a8cf6ea822ed0a4a1d7cd2c81195"
   },
   "outputs": [],
   "source": [
    "weight_path=\"{}_weights.best.hdf5\".format('stroke_lstm_model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = False)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=1e-6)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=10) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "825b3af8-9451-487b-a1e1-538f2f1489e1",
    "_uuid": "ed2fc26af74aed1a93bbc253d61b72db5a81f5cc"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "hists = []\n",
    "hist = model.fit_generator(\n",
    "    train_datagen, steps_per_epoch=STEPS, epochs=EPOCHS,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    verbose=1, callbacks = callbacks\n",
    ")\n",
    "hists.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit_generator(\n",
    "    train_datagen, steps_per_epoch=STEPS, epochs=EPOCHS,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    verbose=1, callbacks = callbacks\n",
    ")\n",
    "hists.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit_generator(\n",
    "    train_datagen, steps_per_epoch=STEPS, epochs=EPOCHS,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    verbose=1, callbacks = callbacks\n",
    ")\n",
    "hists.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a7eb5b62-cf57-4380-8786-9ddc05be658f",
    "_uuid": "858059b6c16d81f86460bef8fcf595e0d68d12b2"
   },
   "outputs": [],
   "source": [
    "valid_predictions = model.predict(x_valid, batch_size=128, verbose=1)\n",
    "map3 = mapk(valid_df[['y']].values, preds2catids(valid_predictions).values)\n",
    "print('Map3: {:.3f}'.format(map3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "193d66e021e6a0b29975ce42067bf65b10d283d7"
   },
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join(INPUT_DIR, 'test_simplified.csv'))\n",
    "test.head()\n",
    "x_test = df_to_img(test, size)\n",
    "print(test.shape, x_test.shape)\n",
    "print('Test array memory {:.2f} GB'.format(x_test.nbytes / 1024.**3 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(x_test, batch_size=128, verbose=1)\n",
    "\n",
    "top3 = preds2catids(test_predictions)\n",
    "top3.head()\n",
    "top3.shape\n",
    "\n",
    "cats = list_all_categories()\n",
    "id2cat = {k: cat.replace(' ', '_') for k, cat in enumerate(cats)}\n",
    "top3cats = top3.replace(id2cat)\n",
    "top3cats.head()\n",
    "top3cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['word'] = top3cats['a'] + ' ' + top3cats['b'] + ' ' + top3cats['c']\n",
    "submission = test[['key_id', 'word']]\n",
    "submission.to_csv('wavenet_submission.csv', index=False)\n",
    "submission.head()\n",
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = dt.datetime.now()\n",
    "print('Latest run {}.\\nTotal time {}s'.format(end, (end - start).seconds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
