{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoencoder",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "DLUVe7lkvl7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd5d8923-e290-44e3-f9e5-1400fc8c652b"
      },
      "cell_type": "code",
      "source": [
        "#@title Code: Import Dependencies {display-mode: \"form\"}\n",
        "\n",
        "# This code will be hidden when the notebook is loaded.\n",
        "\n",
        "!pip install dask[bag] --upgrade > /dev/null\n",
        "%matplotlib inline\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "import os\n",
        "import ast\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [16, 10]\n",
        "plt.rcParams['font.size'] = 14\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, Input, MaxPooling2D, UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from keras.applications import MobileNet\n",
        "from keras.applications.mobilenet import preprocess_input\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from PIL import Image, ImageDraw, ImageOps\n",
        "from dask import bag\n",
        "start = dt.datetime.now()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hb4aPN-AvoFE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Code: Retrieve Data from Google Drive {display-mode: \"form\"}\n",
        "\n",
        "# This code will be hidden when the notebook is loaded.\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "def download_drive_dir(local_dir, folder_id, whitelist=[]):\n",
        "    \"\"\"\n",
        "    params:\n",
        "        local_dir: Colaboratory directory\n",
        "        folder_id: Google Drive folder ID\n",
        "    \"\"\"\n",
        "    local_download_path = os.path.expanduser(local_dir) \n",
        "    try:\n",
        "        os.makedirs(local_download_path)\n",
        "    except Exception as e:\n",
        "        print('Error creating path:', e)\n",
        "\n",
        "    file_list = drive.ListFile(\n",
        "        {'q': \"'{}' in parents\".format(folder_id)}).GetList()\n",
        "\n",
        "    for f in tqdm(file_list):\n",
        "        if whitelist and f['title'] in whitelist:\n",
        "            # print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "            fname = os.path.join(local_download_path, f['title'])\n",
        "            # print('downloading to {}'.format(fname))\n",
        "            f_ = drive.CreateFile({'id': f['id']})\n",
        "            f_.GetContentFile(fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VrNCavf1pev5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91f8825d-4052-4bff-b597-527770fa45b5"
      },
      "cell_type": "code",
      "source": [
        "download_drive_dir('./input/shuffle-csvs', '1H8ogDcbBGsgAJkxaOxXd-XR3ZH14su3i')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 70374.23it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "J5ADsokGo5NO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5ab6b1f5-35f1-4c25-8205-65504e96867e"
      },
      "cell_type": "code",
      "source": [
        "CATLIST = [\"car.csv\"]\n",
        "download_drive_dir('./input/train-simplified', '1ChgkDM2cfJF-WqLqK2untxOl_2vD5WfT', CATLIST)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error creating path: [Errno 17] File exists: './input/train-simplified'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 340/340 [00:01<00:00, 249.59it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "GTyaJL25-Kr5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "valfrac = 0.1\n",
        "num_classes = len(CATLIST)\n",
        "classfiles = os.listdir('./input/train-simplified')\n",
        "numstonames = {i: v[:-4].replace(\" \", \"_\") for i, v in enumerate(classfiles)}\n",
        "imheight, imwidth = 28, 28\n",
        "ims_per_class = 100000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kio2wxZd_rlx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "cellView": "code",
        "outputId": "3270312a-103e-4f7a-f977-c8f5e23a597e"
      },
      "cell_type": "code",
      "source": [
        "#@title Code: Generate Data - Few Categories - X_train, X_val\n",
        "# Conversion to image from sequence\n",
        "\n",
        "import cv2 as cv\n",
        "\n",
        "def draw_it(strokes):\n",
        "    image = Image.new(\"P\", (259,259), color=255)\n",
        "    image_draw = ImageDraw.Draw(image)\n",
        "    for stroke in ast.literal_eval(strokes):\n",
        "        for i in range(len(stroke[0])-1):\n",
        "            image_draw.line([stroke[0][i], \n",
        "                             stroke[1][i],\n",
        "                             stroke[0][i+1], \n",
        "                             stroke[1][i+1]],\n",
        "                            fill=0, width=6)\n",
        "    res = cv.resize(np.array(image), (imheight, imwidth), interpolation=cv.INTER_AREA)\n",
        "    return res/255.\n",
        "\n",
        "# Get training and testing data\n",
        "train_grand = []\n",
        "class_paths = glob('./input/train-simplified/*.csv')\n",
        "for i,c in enumerate(tqdm(class_paths[0: num_classes])):\n",
        "    \n",
        "    train = pd.read_csv(c, usecols=['drawing', 'recognized'], nrows=ims_per_class*5//4)\n",
        "    train = train[train.recognized == True].head(ims_per_class)\n",
        "    imagebag = bag.from_sequence(train.drawing.values).map(draw_it)\n",
        "    \n",
        "    trainarray = np.array(imagebag.compute()) \n",
        "    trainarray = np.reshape(trainarray, (ims_per_class, -1))    \n",
        "    labelarray = np.full((train.shape[0], 1), i)\n",
        "    trainarray = np.concatenate((labelarray, trainarray), axis=1)\n",
        "    train_grand.append(trainarray)\n",
        "    \n",
        "train_grand = np.array([train_grand.pop() for i in np.arange(num_classes)])\n",
        "train_grand = train_grand.reshape((-1, (imheight*imwidth+1)))\n",
        "\n",
        "del trainarray\n",
        "del train\n",
        "\n",
        "cutpt = int(valfrac * train_grand.shape[0])\n",
        "\n",
        "np.random.shuffle(train_grand)\n",
        "y_train, X_train = train_grand[cutpt: , 0], train_grand[cutpt: , 1:]\n",
        "y_val, X_val = train_grand[0:cutpt, 0], train_grand[0:cutpt, 1:]\n",
        "\n",
        "del train_grand\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "X_train = X_train.reshape(X_train.shape[0], imheight, imwidth, 1)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "X_val = X_val.reshape(X_val.shape[0], imheight, imwidth, 1)\n",
        "\n",
        "print(y_train.shape, \"\\n\",\n",
        "      X_train.shape, \"\\n\",\n",
        "      y_val.shape, \"\\n\",\n",
        "      X_val.shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [01:26<00:00, 86.26s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(90000, 1) \n",
            " (90000, 28, 28, 1) \n",
            " (10000, 1) \n",
            " (10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2IhBk4knv5uk",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DP_DIR = './input/shuffle-csvs/'\n",
        "INPUT_DIR = './input/'\n",
        "BASE_SIZE = 256\n",
        "NCSVS = 100\n",
        "NCATS = 340\n",
        "np.random.seed(seed=1987)\n",
        "tf.set_random_seed(seed=1987)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aX390QD7v9HS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Code: Define Metrics {display-mode: \"form\"}\n",
        "\n",
        "# This code will be hidden when the notebook is loaded.\n",
        "\n",
        "def f2cat(filename):\n",
        "    return filename.split('.')[0]\n",
        "\n",
        "def list_all_categories():\n",
        "    files = os.listdir(os.path.join(INPUT_DIR, 'train_simplified'))\n",
        "    return sorted([f2cat(f) for f in files], key=str.lower)\n",
        "\n",
        "def apk(actual, predicted, k=3):\n",
        "    \"\"\"\n",
        "    Source: https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
        "    \"\"\"\n",
        "    if len(predicted) > k:\n",
        "        predicted = predicted[:k]\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "def mapk(actual, predicted, k=3):\n",
        "    \"\"\"\n",
        "    Source: https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
        "    \"\"\"\n",
        "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
        "\n",
        "def preds2catids(predictions):\n",
        "    return pd.DataFrame(np.argsort(-predictions, axis=1)[:, :3], columns=['a', 'b', 'c'])\n",
        "\n",
        "def top_3_accuracy(y_true, y_pred):\n",
        "    return top_k_categorical_accuracy(y_true, y_pred, k=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W0DARFLZ80dW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "STEPS = 100\n",
        "EPOCHS = 15\n",
        "size = 28\n",
        "batchsize = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NOW0033V8rJQ",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "#@title Code: Image generator\n",
        "def draw_cv2(raw_strokes, size=256, lw=12, time_color=True):\n",
        "    img = np.zeros((BASE_SIZE, BASE_SIZE), np.uint8)\n",
        "    for t, stroke in enumerate(raw_strokes):\n",
        "        for i in range(len(stroke[0]) - 1):\n",
        "            color = 255 - min(t, 10) * 13 if time_color else 255\n",
        "            _ = cv2.line(img, (stroke[0][i], stroke[1][i]),\n",
        "                         (stroke[0][i + 1], stroke[1][i + 1]), color, lw)\n",
        "    if size != BASE_SIZE:\n",
        "        return cv2.resize(img, (size, size))\n",
        "    else:\n",
        "        return img\n",
        "\n",
        "def image_generator_xd(size, batchsize, ks, lw=12, time_color=True):\n",
        "    while True:\n",
        "        for k in np.random.permutation(ks):\n",
        "            filename = os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(k))\n",
        "            for df in pd.read_csv(filename, chunksize=batchsize):\n",
        "                df['drawing'] = df['drawing'].apply(ast.literal_eval)\n",
        "                x = np.zeros((len(df), size, size, 1))\n",
        "                for i, raw_strokes in enumerate(df.drawing.values):\n",
        "                    x[i, :, :, 0] = draw_cv2(raw_strokes, size=size, lw=lw,\n",
        "                                             time_color=time_color)\n",
        "                x = preprocess_input(x).astype(np.float32)\n",
        "                y = keras.utils.to_categorical(df.y, num_classes=NCATS)\n",
        "                yield x, x\n",
        "\n",
        "def df_to_image_array_xd(df, size, lw=12, time_color=True):\n",
        "    df['drawing'] = df['drawing'].apply(ast.literal_eval)\n",
        "    x = np.zeros((len(df), size, size, 1))\n",
        "    for i, raw_strokes in enumerate(df.drawing.values):\n",
        "        x[i, :, :, 0] = draw_cv2(raw_strokes, size=size, lw=lw, time_color=time_color)\n",
        "    x = preprocess_input(x).astype(np.float32)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cuSOzyAx8wa_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "valid_df = pd.read_csv(os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(NCSVS - 1)), nrows=6000)\n",
        "x_valid = df_to_image_array_xd(valid_df, size)\n",
        "y_valid = keras.utils.to_categorical(valid_df.y, num_classes=NCATS)\n",
        "print(x_valid.shape, y_valid.shape)\n",
        "print('Validation array memory {:.2f} GB'.format(x_valid.nbytes / 1024.**3 ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "96VXbWjS83ce",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_datagen = image_generator_xd(size=size, batchsize=batchsize, ks=range(NCSVS - 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cQ8qzeE3Ra8X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, Lambda\n",
        "from keras.models import Model\n",
        "from keras.layers import Reshape\n",
        "from keras import backend as K\n",
        "from keras import objectives"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4zVMOn0vHkcz",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title conv_autoencoder_1\n",
        "def conv_autoencoder_1(size):\n",
        "    input_img = Input(shape=(size, size, 1))  # adapt this if using `channels_first` image data format\n",
        "\n",
        "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Flatten()(x)\n",
        "    encoding = Dense(10)(x)\n",
        "    # at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "    x = Dense(128)(encoding)\n",
        "    x = Reshape(target_shape=(4, 4, 8))(x)\n",
        "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    encoder = Model(input_img, encoding)\n",
        "    autoencoder = Model(input_img, decoded)\n",
        "    \n",
        "    return encoder, autoencoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SuQRnKqFRtgc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_autoencoder_2(size):\n",
        "    input_img = Input(shape=(size, size, 1))  # adapt this if using `channels_first` image data format\n",
        "    x = Conv2D(64, (7, 7), activation='relu', padding='same')(input_img)\n",
        "    x = Conv2D(64, (7, 7), activation='relu', padding='same')(x)\n",
        "    x = Conv2D(64, (7, 7), strides=(2,2), activation='relu', padding='same')(x) # 12 x 12 x 32\n",
        "    x = Conv2D(32, (5, 5), strides=(2,2), activation='relu', padding='same')(x) # 12 x 12 x 32\n",
        "    x = Conv2D(16, (3, 3), strides=(2,2), activation='relu', padding='same')(x) # 4 x 4 x 16\n",
        "    x = Flatten()(x)\n",
        "    encoding = Dense(10)(x)\n",
        "    x = Dense(784)(encoding)\n",
        "    x = Reshape(target_shape=(7, 7, 16))(x)\n",
        "    x = Conv2DTranspose(16, (3, 3), strides=(2,2), activation='relu', padding='same')(x)\n",
        "    x = Conv2DTranspose(32, (5, 5), strides=(2,2), activation='relu', padding='same')(x)\n",
        "    x = Conv2DTranspose(64, (7, 7), strides=(2,2), activation='relu', padding='same')(x)\n",
        "    x = Conv2DTranspose(64, (7, 7), activation='relu', padding='same')(x)\n",
        "    x = Conv2DTranspose(64, (7, 7), activation='relu', padding='same')(x)\n",
        "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "    \n",
        "    \n",
        "    encoder = Model(input_img, encoding)\n",
        "    autoencoder = Model(input_img, decoded)\n",
        "    \n",
        "    return encoder, autoencoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_AibyCIfwuwW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = batchsize\n",
        "original_dim = 28*28\n",
        "latent_dim = 128\n",
        "\n",
        "def sampling(args):\n",
        "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
        "    # Arguments:\n",
        "        args (tensor): mean and log of variance of Q(z|X)\n",
        "    # Returns:\n",
        "        z (tensor): sampled latent vector\n",
        "    \"\"\"\n",
        "\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean=0 and std=1.0\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "def variational_autoencoder(size):\n",
        "    input_img = Input(shape=(size, size, 1))\n",
        "    x = Conv2D(4, (2, 2), activation='relu', padding='same')(input_img)\n",
        "    x = Conv2D(4, (2, 2), activation='relu', padding='same')(x)\n",
        "#     x = Conv2D(4, (2, 2), strides=(2,2), activation='relu', padding='same')(x)\n",
        "    x = Conv2D(8, (2, 2), activation='relu', padding='same')(x)\n",
        "    x = Conv2D(8, (2, 2), activation='relu', padding='same')(x)\n",
        "    x = Conv2D(8, (5, 5), strides=(2,2), activation='relu', padding='same')(x)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    z_mean = Dense(latent_dim)(x)\n",
        "    z_log_sigma = Dense(latent_dim)(x)\n",
        "\n",
        "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "    # so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
        "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
        "\n",
        "    def decoder_convs(x):\n",
        "        x = Dense(256, activation='relu')(x)\n",
        "        x = Dense(original_dim)(x)\n",
        "        x = Reshape(target_shape=(7, 7, 16))(x)\n",
        "        x = Conv2DTranspose(8, (5, 5), strides=(2,2), activation='relu', padding='same')(x)\n",
        "        x = Conv2D(8, (2, 2), activation='relu', padding='same')(x)\n",
        "        x = Conv2D(8, (2, 2), activation='relu', padding='same')(x)\n",
        "        x = Conv2DTranspose(4, (2, 2), strides=(2,2), activation='relu', padding='same')(x)\n",
        "        x = Conv2D(4, (2, 2), activation='relu', padding='same')(x)\n",
        "        x = Conv2D(4, (2, 2), activation='relu', padding='same')(x)\n",
        "        x = Conv2D(1, (2, 2), activation='sigmoid', padding='same')(x)\n",
        "        x = Reshape(target_shape=(28, 28, 1))(x)\n",
        "        return x\n",
        "    \n",
        "    x_decoded_mean = decoder_convs(z)\n",
        "    \n",
        "    # end-to-end autoencoder\n",
        "    vae = Model(input_img, x_decoded_mean)\n",
        "\n",
        "    # encoder, from inputs to latent space\n",
        "    encoder = Model(input_img, z_mean)\n",
        "\n",
        "    # generator, from latent space to reconstructed inputs\n",
        "    decoder_input = Input(shape=(latent_dim,))\n",
        "    _x_decoded_mean = decoder_convs(decoder_input)\n",
        "    generator = Model(decoder_input, _x_decoded_mean)\n",
        "    \n",
        "    def vae_loss(x, x_decoded_mean):\n",
        "        xent_loss = K.mean(objectives.binary_crossentropy(x, x_decoded_mean), axis=(1, 2))\n",
        "        print(xent_loss.shape)\n",
        "        kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
        "        print(kl_loss.shape)\n",
        "        return xent_loss + kl_loss\n",
        "\n",
        "    return vae, encoder, generator, vae_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3tBLg0SNxwg1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vae, encoder, generator, vae_loss = variational_autoencoder(size)\n",
        "vae.compile(optimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0), loss=vae_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6iUqNhiSBRD0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vae.fit(X_train, X_train, epochs=120, batch_size=512, shuffle=True, validation_data=(X_val, X_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qKRbCwmjIHeN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "cellView": "form",
        "outputId": "b034e96e-3aa2-4b24-a571-297628b37d5d"
      },
      "cell_type": "code",
      "source": [
        "#@title Code: Import MNIST Data - x_train, x_test\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SebgERZkgu56",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4131
        },
        "outputId": "4e3af57d-caf4-460e-e1d0-12fa8beed830"
      },
      "cell_type": "code",
      "source": [
        "vae.fit(x_train, x_train, epochs=120, batch_size=512, shuffle=True, validation_data=(x_test, x_test))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/120\n",
            "60000/60000 [==============================] - 7s 109us/step - loss: 0.3539 - val_loss: 0.2934\n",
            "Epoch 2/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2737 - val_loss: 0.2586\n",
            "Epoch 3/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2542 - val_loss: 0.2513\n",
            "Epoch 4/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2499 - val_loss: 0.2477\n",
            "Epoch 5/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2450 - val_loss: 0.2414\n",
            "Epoch 6/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2391 - val_loss: 0.2389\n",
            "Epoch 7/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2344 - val_loss: 0.2340\n",
            "Epoch 8/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2312 - val_loss: 0.2317\n",
            "Epoch 9/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2285 - val_loss: 0.2277\n",
            "Epoch 10/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2273 - val_loss: 0.2291\n",
            "Epoch 11/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2259 - val_loss: 0.2245\n",
            "Epoch 12/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2239 - val_loss: 0.2280\n",
            "Epoch 13/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2230 - val_loss: 0.2229\n",
            "Epoch 14/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2222 - val_loss: 0.2236\n",
            "Epoch 15/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2214 - val_loss: 0.2209\n",
            "Epoch 16/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2204 - val_loss: 0.2202\n",
            "Epoch 17/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2197 - val_loss: 0.2200\n",
            "Epoch 18/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2193 - val_loss: 0.2173\n",
            "Epoch 19/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2188 - val_loss: 0.2176\n",
            "Epoch 20/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2185 - val_loss: 0.2172\n",
            "Epoch 21/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2182 - val_loss: 0.2194\n",
            "Epoch 22/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2179 - val_loss: 0.2190\n",
            "Epoch 23/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2178 - val_loss: 0.2181\n",
            "Epoch 24/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2173 - val_loss: 0.2196\n",
            "Epoch 25/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2172 - val_loss: 0.2180\n",
            "Epoch 26/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2170 - val_loss: 0.2169\n",
            "Epoch 27/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2167 - val_loss: 0.2167\n",
            "Epoch 28/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2166 - val_loss: 0.2176\n",
            "Epoch 29/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2163 - val_loss: 0.2162\n",
            "Epoch 30/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2161 - val_loss: 0.2168\n",
            "Epoch 31/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2160 - val_loss: 0.2168\n",
            "Epoch 32/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2159 - val_loss: 0.2165\n",
            "Epoch 33/120\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.2158 - val_loss: 0.2151\n",
            "Epoch 34/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2156 - val_loss: 0.2142\n",
            "Epoch 35/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2155 - val_loss: 0.2141\n",
            "Epoch 36/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2152 - val_loss: 0.2152\n",
            "Epoch 37/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2150 - val_loss: 0.2148\n",
            "Epoch 38/120\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.2149 - val_loss: 0.2144\n",
            "Epoch 39/120\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.2148 - val_loss: 0.2186\n",
            "Epoch 40/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2147 - val_loss: 0.2145\n",
            "Epoch 41/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2145 - val_loss: 0.2162\n",
            "Epoch 42/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2146 - val_loss: 0.2140\n",
            "Epoch 43/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2143 - val_loss: 0.2133\n",
            "Epoch 44/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2142 - val_loss: 0.2138\n",
            "Epoch 45/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2141 - val_loss: 0.2147\n",
            "Epoch 46/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2140 - val_loss: 0.2141\n",
            "Epoch 47/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2138 - val_loss: 0.2151\n",
            "Epoch 48/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2140 - val_loss: 0.2151\n",
            "Epoch 49/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2137 - val_loss: 0.2179\n",
            "Epoch 50/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2136 - val_loss: 0.2145\n",
            "Epoch 51/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2136 - val_loss: 0.2131\n",
            "Epoch 52/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2136 - val_loss: 0.2156\n",
            "Epoch 53/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2134 - val_loss: 0.2135\n",
            "Epoch 54/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2132 - val_loss: 0.2132\n",
            "Epoch 55/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2131 - val_loss: 0.2131\n",
            "Epoch 56/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2130 - val_loss: 0.2130\n",
            "Epoch 57/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2130 - val_loss: 0.2135\n",
            "Epoch 58/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2130 - val_loss: 0.2123\n",
            "Epoch 59/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2128 - val_loss: 0.2124\n",
            "Epoch 60/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2128 - val_loss: 0.2124\n",
            "Epoch 61/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2129 - val_loss: 0.2121\n",
            "Epoch 62/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2127 - val_loss: 0.2176\n",
            "Epoch 63/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2127 - val_loss: 0.2110\n",
            "Epoch 64/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2126 - val_loss: 0.2125\n",
            "Epoch 65/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2126 - val_loss: 0.2121\n",
            "Epoch 66/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2125 - val_loss: 0.2120\n",
            "Epoch 67/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2124 - val_loss: 0.2123\n",
            "Epoch 68/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2125 - val_loss: 0.2137\n",
            "Epoch 69/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2125 - val_loss: 0.2114\n",
            "Epoch 70/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2123 - val_loss: 0.2120\n",
            "Epoch 71/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2123 - val_loss: 0.2123\n",
            "Epoch 72/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2123 - val_loss: 0.2118\n",
            "Epoch 73/120\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.2121 - val_loss: 0.2141\n",
            "Epoch 74/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2120 - val_loss: 0.2129\n",
            "Epoch 75/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2121 - val_loss: 0.2122\n",
            "Epoch 76/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2121 - val_loss: 0.2120\n",
            "Epoch 77/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2119 - val_loss: 0.2116\n",
            "Epoch 78/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2120 - val_loss: 0.2140\n",
            "Epoch 79/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2120 - val_loss: 0.2123\n",
            "Epoch 80/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2121 - val_loss: 0.2140\n",
            "Epoch 81/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2118 - val_loss: 0.2113\n",
            "Epoch 82/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2120 - val_loss: 0.2128\n",
            "Epoch 83/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2118 - val_loss: 0.2117\n",
            "Epoch 84/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2118 - val_loss: 0.2122\n",
            "Epoch 85/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2118 - val_loss: 0.2112\n",
            "Epoch 86/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2119 - val_loss: 0.2117\n",
            "Epoch 87/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2118 - val_loss: 0.2111\n",
            "Epoch 88/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2117 - val_loss: 0.2107\n",
            "Epoch 89/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2117 - val_loss: 0.2117\n",
            "Epoch 90/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2115 - val_loss: 0.2106\n",
            "Epoch 91/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2115 - val_loss: 0.2117\n",
            "Epoch 92/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2115 - val_loss: 0.2117\n",
            "Epoch 93/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2115 - val_loss: 0.2111\n",
            "Epoch 94/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2114 - val_loss: 0.2127\n",
            "Epoch 95/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2116 - val_loss: 0.2130\n",
            "Epoch 96/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2114 - val_loss: 0.2128\n",
            "Epoch 97/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2113 - val_loss: 0.2110\n",
            "Epoch 98/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2113 - val_loss: 0.2147\n",
            "Epoch 99/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2112 - val_loss: 0.2106\n",
            "Epoch 100/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2114 - val_loss: 0.2120\n",
            "Epoch 101/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2112 - val_loss: 0.2118\n",
            "Epoch 102/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2112 - val_loss: 0.2132\n",
            "Epoch 103/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2112 - val_loss: 0.2100\n",
            "Epoch 104/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2111 - val_loss: 0.2118\n",
            "Epoch 105/120\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.2113 - val_loss: 0.2109\n",
            "Epoch 106/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2111 - val_loss: 0.2119\n",
            "Epoch 107/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2111 - val_loss: 0.2132\n",
            "Epoch 108/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2112 - val_loss: 0.2118\n",
            "Epoch 109/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2110 - val_loss: 0.2123\n",
            "Epoch 110/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2111 - val_loss: 0.2108\n",
            "Epoch 111/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2111 - val_loss: 0.2114\n",
            "Epoch 112/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2111 - val_loss: 0.2126\n",
            "Epoch 113/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2110 - val_loss: 0.2100\n",
            "Epoch 114/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2110 - val_loss: 0.2114\n",
            "Epoch 115/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2109 - val_loss: 0.2118\n",
            "Epoch 116/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2109 - val_loss: 0.2112\n",
            "Epoch 117/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2109 - val_loss: 0.2127\n",
            "Epoch 118/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2109 - val_loss: 0.2106\n",
            "Epoch 119/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2107 - val_loss: 0.2106\n",
            "Epoch 120/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2109 - val_loss: 0.2122\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0736fef748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "ELKRSfc-jsnD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4131
        },
        "outputId": "28b1cc7d-1a43-413c-f5e4-5436f6a6076c"
      },
      "cell_type": "code",
      "source": [
        "vae.fit(x_train, x_train, epochs=120, batch_size=512, shuffle=True, validation_data=(x_test, x_test))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/120\n",
            "60000/60000 [==============================] - 5s 91us/step - loss: 0.2108 - val_loss: 0.2099\n",
            "Epoch 2/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2109 - val_loss: 0.2107\n",
            "Epoch 3/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2109 - val_loss: 0.2106\n",
            "Epoch 4/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2108 - val_loss: 0.2113\n",
            "Epoch 5/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2110 - val_loss: 0.2104\n",
            "Epoch 6/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2108 - val_loss: 0.2100\n",
            "Epoch 7/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2108 - val_loss: 0.2116\n",
            "Epoch 8/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2109 - val_loss: 0.2120\n",
            "Epoch 9/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2107 - val_loss: 0.2106\n",
            "Epoch 10/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2107 - val_loss: 0.2117\n",
            "Epoch 11/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2107 - val_loss: 0.2114\n",
            "Epoch 12/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2106 - val_loss: 0.2105\n",
            "Epoch 13/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2107 - val_loss: 0.2109\n",
            "Epoch 14/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2107 - val_loss: 0.2107\n",
            "Epoch 15/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2105 - val_loss: 0.2107\n",
            "Epoch 16/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2105 - val_loss: 0.2140\n",
            "Epoch 17/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2106 - val_loss: 0.2115\n",
            "Epoch 18/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2105 - val_loss: 0.2118\n",
            "Epoch 19/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2105 - val_loss: 0.2102\n",
            "Epoch 20/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2107 - val_loss: 0.2112\n",
            "Epoch 21/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2104 - val_loss: 0.2116\n",
            "Epoch 22/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2104 - val_loss: 0.2097\n",
            "Epoch 23/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2105 - val_loss: 0.2115\n",
            "Epoch 24/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2106 - val_loss: 0.2107\n",
            "Epoch 25/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2104 - val_loss: 0.2101\n",
            "Epoch 26/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2103 - val_loss: 0.2119\n",
            "Epoch 27/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2104 - val_loss: 0.2102\n",
            "Epoch 28/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2104 - val_loss: 0.2101\n",
            "Epoch 29/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2104 - val_loss: 0.2105\n",
            "Epoch 30/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2104 - val_loss: 0.2102\n",
            "Epoch 31/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2103 - val_loss: 0.2097\n",
            "Epoch 32/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2103 - val_loss: 0.2106\n",
            "Epoch 33/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2104 - val_loss: 0.2110\n",
            "Epoch 34/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2103 - val_loss: 0.2112\n",
            "Epoch 35/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2104 - val_loss: 0.2104\n",
            "Epoch 36/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2101 - val_loss: 0.2109\n",
            "Epoch 37/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2105 - val_loss: 0.2112\n",
            "Epoch 38/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2102 - val_loss: 0.2109\n",
            "Epoch 39/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2102 - val_loss: 0.2096\n",
            "Epoch 40/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2101 - val_loss: 0.2108\n",
            "Epoch 41/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2103 - val_loss: 0.2134\n",
            "Epoch 42/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2103 - val_loss: 0.2098\n",
            "Epoch 43/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2102 - val_loss: 0.2118\n",
            "Epoch 44/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2102 - val_loss: 0.2104\n",
            "Epoch 45/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2101 - val_loss: 0.2101\n",
            "Epoch 46/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2103 - val_loss: 0.2100\n",
            "Epoch 47/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2101 - val_loss: 0.2096\n",
            "Epoch 48/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2101 - val_loss: 0.2114\n",
            "Epoch 49/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2102 - val_loss: 0.2115\n",
            "Epoch 50/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2102 - val_loss: 0.2100\n",
            "Epoch 51/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2102 - val_loss: 0.2108\n",
            "Epoch 52/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2100 - val_loss: 0.2107\n",
            "Epoch 53/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2101 - val_loss: 0.2112\n",
            "Epoch 54/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2101 - val_loss: 0.2108\n",
            "Epoch 55/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2100 - val_loss: 0.2127\n",
            "Epoch 56/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2101 - val_loss: 0.2106\n",
            "Epoch 57/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2101 - val_loss: 0.2089\n",
            "Epoch 58/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2101 - val_loss: 0.2104\n",
            "Epoch 59/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2099 - val_loss: 0.2109\n",
            "Epoch 60/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2101 - val_loss: 0.2110\n",
            "Epoch 61/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2101 - val_loss: 0.2100\n",
            "Epoch 62/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2099 - val_loss: 0.2108\n",
            "Epoch 63/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2098 - val_loss: 0.2125\n",
            "Epoch 64/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2101 - val_loss: 0.2103\n",
            "Epoch 65/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2100 - val_loss: 0.2107\n",
            "Epoch 66/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2099 - val_loss: 0.2109\n",
            "Epoch 67/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2098 - val_loss: 0.2094\n",
            "Epoch 68/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2098 - val_loss: 0.2090\n",
            "Epoch 69/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2099 - val_loss: 0.2133\n",
            "Epoch 70/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2098 - val_loss: 0.2108\n",
            "Epoch 71/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2100 - val_loss: 0.2103\n",
            "Epoch 72/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2101 - val_loss: 0.2094\n",
            "Epoch 73/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2099 - val_loss: 0.2099\n",
            "Epoch 74/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2099 - val_loss: 0.2102\n",
            "Epoch 75/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2098 - val_loss: 0.2099\n",
            "Epoch 76/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2098 - val_loss: 0.2102\n",
            "Epoch 77/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2099 - val_loss: 0.2098\n",
            "Epoch 78/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2099 - val_loss: 0.2100\n",
            "Epoch 79/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2098 - val_loss: 0.2095\n",
            "Epoch 80/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2098 - val_loss: 0.2103\n",
            "Epoch 81/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2098 - val_loss: 0.2122\n",
            "Epoch 82/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2097 - val_loss: 0.2137\n",
            "Epoch 83/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2098 - val_loss: 0.2094\n",
            "Epoch 84/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2097 - val_loss: 0.2099\n",
            "Epoch 85/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2098 - val_loss: 0.2090\n",
            "Epoch 86/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2096 - val_loss: 0.2135\n",
            "Epoch 87/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2097 - val_loss: 0.2097\n",
            "Epoch 88/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2097 - val_loss: 0.2107\n",
            "Epoch 89/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2097 - val_loss: 0.2095\n",
            "Epoch 90/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2096 - val_loss: 0.2100\n",
            "Epoch 91/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2097 - val_loss: 0.2103\n",
            "Epoch 92/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2096 - val_loss: 0.2100\n",
            "Epoch 93/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2096 - val_loss: 0.2098\n",
            "Epoch 94/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2097 - val_loss: 0.2104\n",
            "Epoch 95/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2096 - val_loss: 0.2117\n",
            "Epoch 96/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2096 - val_loss: 0.2091\n",
            "Epoch 97/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2096 - val_loss: 0.2097\n",
            "Epoch 98/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2095 - val_loss: 0.2107\n",
            "Epoch 99/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2097 - val_loss: 0.2092\n",
            "Epoch 100/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2095 - val_loss: 0.2090\n",
            "Epoch 101/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2096 - val_loss: 0.2108\n",
            "Epoch 102/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2095 - val_loss: 0.2100\n",
            "Epoch 103/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2095 - val_loss: 0.2095\n",
            "Epoch 104/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2096 - val_loss: 0.2102\n",
            "Epoch 105/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2095 - val_loss: 0.2108\n",
            "Epoch 106/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2096 - val_loss: 0.2087\n",
            "Epoch 107/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2095 - val_loss: 0.2101\n",
            "Epoch 108/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2095 - val_loss: 0.2094\n",
            "Epoch 109/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2096 - val_loss: 0.2113\n",
            "Epoch 110/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2095 - val_loss: 0.2098\n",
            "Epoch 111/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2095 - val_loss: 0.2106\n",
            "Epoch 112/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2095 - val_loss: 0.2102\n",
            "Epoch 113/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2095 - val_loss: 0.2105\n",
            "Epoch 114/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2094 - val_loss: 0.2108\n",
            "Epoch 115/120\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2095 - val_loss: 0.2086\n",
            "Epoch 116/120\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.2095 - val_loss: 0.2099\n",
            "Epoch 117/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2095 - val_loss: 0.2092\n",
            "Epoch 118/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2093 - val_loss: 0.2103\n",
            "Epoch 119/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2094 - val_loss: 0.2106\n",
            "Epoch 120/120\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2095 - val_loss: 0.2090\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0730206828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "MjK30dARIQop",
        "colab_type": "code",
        "outputId": "9c30a1cc-e5f4-4403-8ce0-94ae8b2acaef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# load weights into new model\n",
        "vae.load_weights(\"mnist_vae28_2_240epochs.h5\")\n",
        "\n",
        "print(\"Loaded model from disk\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tT7hrwQ_I-KU",
        "colab_type": "code",
        "outputId": "0846a44c-2095-4430-b8a6-9e89658e8b75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "decoded_imgs = vae.predict(x_test)\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(1, n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i)\n",
        "    plt.imshow(x_test[i].reshape(size, size))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(size, size))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0735f0cf98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071cd1e4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071ccd77b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071cc8fb70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071cc3eef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071cc54518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071cbbb9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071cbf1da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071cba6c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071cb472e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071cb244a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071cadb860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071cb33470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071caae128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071ca02e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071c9c4240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071c9f97b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071c9ad400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f071c968828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/gAAADjCAYAAAAxIr9SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngBd8AAAAAAAS8P9Jw9BAhx0l5QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0735f0cf98>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "knvQFhRnnSac",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "decoded_imgs = vae.predict(x_test)\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(1, n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i)\n",
        "    plt.imshow(x_test[i].reshape(size, size))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(size, size))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "44arYUbzTUou",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Save models\"\"\"\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# serialize model to JSON\n",
        "model_json = head_model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "files.download('model.json') \n",
        "    \n",
        "# serialize weights to HDF5\n",
        "head_model.save_weights(\"model.h5\")\n",
        "files.download('model.h5')\n",
        "\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6UOM_54KynH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yfk5vixKLFqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoded = encoder.predict(x_valid)\n",
        "print(encoded.shape)\n",
        "kmeans.fit(encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v29c7E2IMz89",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels = kmeans.fit_predict(encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WcY8GNi9NuuC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('proj_out_vals.tsv', 'w') as f:\n",
        "    for i in range(len(encoded)):\n",
        "        row = '\\t'.join([str(x) for x in encoded[i, :].tolist()]) + '\\n'\n",
        "        f.write(row)\n",
        "\n",
        "\n",
        "with open('proj_out_labs.tsv', 'w') as f:\n",
        "    for i in range(len(labels)):\n",
        "        row = '{}\\n'.format(str(labels[i]))\n",
        "        f.write(row)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}